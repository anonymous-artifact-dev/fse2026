This is a page for result presenting the computational time.
Time ~ Time.7 indicates each datasets (App, SO, GitHub, Jira, Gerrit, Tweets_Full, Tweets_n, Tweets_p)

## SLM

| Section | Classifier | DA Method | Time | Time.1 | Time.2 | Time.3 | Time.4 | Time.5 | Time.6 | Time.7 |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| SLM | DeBERTa | No Aug | 116.41 | 552.04 | 421.58 | 828.28 | 311.62 | 170.6 | 136.27 | 155.52 |
| SLM | DeBERTa | SSMBA | 329.39 | 3904.47 | 3375.24 | 5007.18 | 1454.25 | 944.46 | 422.52 | 586.83 |
| SLM | DeBERTa | AEDA | 359.68 | 3865.92 | 2987.24 | 5134.96 | 1611.72 | 830.23 | 385.19 | 517.34 |
| SLM | DeBERTa | C2L | 352.34 | 5939.3 | 7558.29 | 8163.71 | 1774.71 | 1143.9 | 535.45 | 739.21 |
| SLM | DeBERTa | TextSmoothing | 299.15 | 2098.74 | 2535.61 | 5150.22 | 1309.9 | 899.78 | 411.25 | 560.97 |
| SLM | XLNet | No Aug | 144.77 | 970.48 | 568.05 | 862.11 | 448.99 | 221.13 | 179.52 | 155.16 |
| SLM | XLNet | SSMBA | 379.74 | 4534.03 | 3169.27 | 5876.87 | 1695.58 | 972.27 | 443.38 | 601.24 |
| SLM | XLNet | AEDA | 439.44 | 4116.05 | 3162.24 | 5450.04 | 1709.64 | 866.95 | 395.35 | 536.08 |
| SLM | XLNet | C2L | 552.42 | 4750.83 | 8773.1 | 4832.82 | 2282.28 | 1190.41 | 551.08 | 380.23 |
| SLM | XLNet | TextSmoothing | 386.37 | 3611.52 | 3511.28 | 5535.39 | 1748.41 | 937.83 | 422.1 | 576.44 |
| SLM | T5 | No Aug | 337.33 | 691.76 | 594.66 | 925.18 | 497.01 | 461.9 | 314.27 | 332.07 |
| SLM | T5 | SSMBA | 477.12 | 1905.29 | 1580.0 | 2708.44 | 1071.58 | 642.73 | 353.56 | 427.45 |
| SLM | T5 | AEDA | 426.41 | 1487.15 | 1275.94 | 1968.18 | 853.87 | 530.49 | 328.56 | 385.7 |
| SLM | T5 | C2L | 622.55 | 2537.7 | 5688.47 | 3948.24 | 1674.92 | 637.99 | 368.08 | 738.04 |
| SLM | T5 | TextSmoothing | 353.27 | 1204.93 | 1519.33 | 2093.44 | 820.18 | 592.92 | 321.54 | 370.77 |


## LLM Fine-tuned

| Section | Classifier | DA Method | Time | Time.1 | Time.2 | Time.3 | Time.4 | Time.5 | Time.6 | Time.7 |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| LLM Fine-tuning | CodeGen2.5 | No Aug | 79.31 | 1079.5 | 674.43 | 1441.87 | 350.56 | 182.16 | 86.06 | 114.61 |
| LLM Fine-tuning | CodeGen2.5 | SSMBA | 428.99 | 5046.95 | 3712.57 | 6723.11 | 2021.02 | 1016.24 | 452.9 | 643.94 |
| LLM Fine-tuning | CodeGen2.5 | AEDA | 429.16 | 5043.22 | 3815.96 | 6890.1 | 2006.45 | 1024.48 | 479.79 | 643.11 |
| LLM Fine-tuning | CodeGen2.5 | C2L | 560.88 | 6545.29 | 4873.74 | 8973.53 | 2629.92 | 1359.6 | 606.67 | 857.62 |
| LLM Fine-tuning | CodeGen2.5 | TextSmoothing | 431.93 | 5061.96 | 3882.58 | 6921.46 | 2007.37 | 1031.28 | 467.21 | 643.36 |
| LLM Fine-tuning | Phi-2 | No Aug | 37.7 | 379.09 | 336.97 | 562.9 | 165.86 | 96.88 | 43.95 | 59.67 |
| LLM Fine-tuning | Phi-2 | SSMBA | 247.58 | 2898.23 | 2108.43 | 4153.06 | 1170.55 | 712.91 | 277.79 | 422.97 |
| LLM Fine-tuning | Phi-2 | AEDA | 248.48 | 2874.56 | 2162.31 | 4174.58 | 1153.38 | 649.92 | 320.34 | 407.99 |
| LLM Fine-tuning | Phi-2 | C2L | 324.8 | 3838.23 | 2978.16 | 5703.0 | 1539.82 | 829.41 | 392.05 | 538.34 |
| LLM Fine-tuning | Phi-2 | TextSmoothing | 249.64 | 2906.1 | 2155.4 | 4229.53 | 1211.56 | 644.73 | 299.58 | 392.86 |
| LLM Fine-tuning | DeepSeek | No Aug | 53.42 | 657.44 | 447.69 | 855.91 | 237.03 | 123.59 | 58.36 | 78.61 |
| LLM Fine-tuning | DeepSeek | SSMBA | 375.17 | 4278.92 | 3138.11 | 5741.04 | 1766.2 | 889.39 | 395.25 | 558.07 |
| LLM Fine-tuning | DeepSeek | AEDA | 374.65 | 4277.57 | 3362.25 | 5864.69 | 1767.99 | 901.12 | 407.42 | 558.08 |
| LLM Fine-tuning | DeepSeek | C2L | 495.94 | 5646.48 | 4307.61 | 7597.7 | 2337.33 | 1191.9 | 534.89 | 737.87 |
| LLM Fine-tuning | DeepSeek | TextSmoothing | 375.55 | 4323.1 | 3299.06 | 5890.86 | 1762.99 | 900.8 | 407.81 | 558.92 |


## LLM Prompting

| Section | Classifier | DA Method | Time | Time.1 | Time.2 | Time.3 | Time.4 | Time.5 | Time.6 | Time.7 |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| LLM Prompting | GPT-4.1 nano (baseline) | baseline | 30.00 | 585.83 | 256.26 | 752.43 | 140.75 | 73.55 | 32.18 | 41.69 |
| LLM Prompting (k=1) | GPT-4.1 nano | No Aug | 29.96 | 585.84 | 256.26 | 752.40 | 140.80 | 73.63 | 32.25 | 41.75 |
| LLM Prompting (k=1) | GPT-4.1 nano | SSMBA | 34.03 | 589.87 | 260.28 | 756.42 | 144.78 | 77.58 | 36.21 | 45.68 |
| LLM Prompting (k=1) | GPT-4.1 nano | AEDA | 30.01 | 585.84 | 256.25 | 752.51 | 140.85 | 73.56 | 32.21 | 41.74 |
| LLM Prompting (k=1) | GPT-4.1 nano | C2L | 36.62 | 592.50 | 262.89 | 759.17 | 147.49 | 80.29 | 38.92 | 48.41 |
| LLM Prompting (k=1) | GPT-4.1 nano | TextSmoothing | 33.24 | 589.07 | 259.46 | 755.68 | 143.99 | 76.80 | 35.48 | 44.89 |
| LLM Prompting (k=2) | GPT-4.1 nano | No Aug | 30.00 | 585.82 | 256.22 | 752.42 | 140.80 | 73.62 | 32.19 | 41.67 |
| LLM Prompting (k=2) | GPT-4.1 nano | SSMBA | 34.02 | 589.89 | 260.26 | 756.51 | 144.76 | 77.64 | 36.22 | 45.72 |
| LLM Prompting (k=2) | GPT-4.1 nano | AEDA | 29.95 | 585.86 | 256.31 | 752.49 | 140.86 | 73.59 | 32.27 | 41.78 |
| LLM Prompting (k=2) | GPT-4.1 nano | C2L | 36.66 | 592.57 | 262.91 | 759.15 | 147.48 | 80.33 | 38.96 | 48.41 |
| LLM Prompting (k=2) | GPT-4.1 nano | TextSmoothing | 33.18 | 589.09 | 259.43 | 755.65 | 144.04 | 76.79 | 35.44 | 44.97 |
| LLM Prompting (k=4) | GPT-4.1 nano | No Aug | 30.01 | 585.83 | 256.23 | 752.42 | 140.75 | 73.59 | 32.18 | 41.74 |
| LLM Prompting (k=4) | GPT-4.1 nano | SSMBA | 34.00 | 589.90 | 260.25 | 756.42 | 144.81 | 77.59 | 36.19 | 45.73 |
| LLM Prompting (k=4) | GPT-4.1 nano | AEDA | 30.02 | 585.90 | 256.29 | 752.50 | 140.81 | 73.65 | 32.21 | 41.76 |
| LLM Prompting (k=4) | GPT-4.1 nano | C2L | 36.65 | 592.53 | 262.95 | 759.13 | 147.52 | 80.25 | 38.87 | 48.35 |
| LLM Prompting (k=4) | GPT-4.1 nano | TextSmoothing | 33.16 | 589.05 | 259.50 | 755.63 | 144.02 | 76.76 | 35.46 | 44.89 |
| LLM Prompting | GPT-5 nano (baseline) | baseline | 142.71 | 2705.60 | 1264.75 | 3689.48 | 756.98 | 569.65 | 268.08 | 380.55 |
| LLM Prompting (k=1) | GPT-5 nano | No Aug | 142.66 | 2705.54 | 1264.72 | 3689.51 | 756.96 | 569.71 | 268.12 | 380.55 |
| LLM Prompting (k=1) | GPT-5 nano | SSMBA | 146.75 | 2709.57 | 1268.78 | 3693.49 | 760.98 | 573.72 | 272.16 | 384.51 |
| LLM Prompting (k=1) | GPT-5 nano | AEDA | 142.73 | 2705.55 | 1264.72 | 3689.56 | 757.00 | 569.70 | 268.15 | 380.54 |
| LLM Prompting (k=1) | GPT-5 nano | C2L | 149.42 | 2712.30 | 1271.37 | 3696.22 | 763.66 | 576.39 | 274.78 | 387.22 |
| LLM Prompting (k=1) | GPT-5 nano | TextSmoothing | 145.91 | 2708.82 | 1267.91 | 3692.74 | 760.15 | 572.92 | 271.37 | 383.78 |
| LLM Prompting (k=2) | GPT-5 nano | No Aug | 142.75 | 2705.60 | 1264.75 | 3689.49 | 757.01 | 569.66 | 268.14 | 380.53 |
| LLM Prompting (k=2) | GPT-5 nano | SSMBA | 146.76 | 2709.63 | 1268.75 | 3693.53 | 761.04 | 573.66 | 272.16 | 384.53 |
| LLM Prompting (k=2) | GPT-5 nano | AEDA | 142.75 | 2705.60 | 1264.72 | 3689.53 | 756.96 | 569.71 | 268.10 | 380.53 |
| LLM Prompting (k=2) | GPT-5 nano | C2L | 149.36 | 2712.28 | 1271.42 | 3696.20 | 763.68 | 576.37 | 274.77 | 387.20 |
| LLM Prompting (k=2) | GPT-5 nano | TextSmoothing | 145.92 | 2708.76 | 1267.92 | 3692.68 | 760.16 | 572.87 | 271.38 | 383.76 |
| LLM Prompting (k=4) | GPT-5 nano | No Aug | 142.65 | 2705.54 | 1264.75 | 3689.49 | 757.00 | 569.62 | 268.11 | 380.56 |
| LLM Prompting (k=4) | GPT-5 nano | SSMBA | 146.69 | 2709.62 | 1268.74 | 3693.47 | 760.98 | 573.70 | 272.12 | 384.54 |
| LLM Prompting (k=4) | GPT-5 nano | AEDA | 142.74 | 2705.60 | 1264.74 | 3689.52 | 757.02 | 569.66 | 268.14 | 380.51 |
| LLM Prompting (k=4) | GPT-5 nano | C2L | 149.41 | 2712.28 | 1271.40 | 3696.17 | 763.69 | 576.39 | 274.83 | 387.18 |
| LLM Prompting (k=4) | GPT-5 nano | TextSmoothing | 145.97 | 2708.83 | 1267.96 | 3692.68 | 760.23 | 572.85 | 271.39 | 383.76 |

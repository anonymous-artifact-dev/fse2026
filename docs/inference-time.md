This is a page for result presenting the computational time (Inference Time and Augmentation + Train Time).


| Section             | Classifier              | DA Method     | App_Avg | Total Time | Inference Time | Augmentation + Train Time | SO_Avg | Total Time | Inference Time | Augmentation + Train Time | GitHub_Avg | Total Time | Inference Time | Augmentation + Train Time | Jira_Avg | Total Time | Inference Time | Augmentation + Train Time | Gerrit_Avg | Total Time | Inference Time | Augmentation + Train Time | TweetsFull_Avg | Total Time | Inference Time | Augmentation + Train Time | TweetsP_Avg | Total Time | Inference Time | Augmentation + Train Time | TweetsN_Avg | Total Time | Inference Time | Augmentation + Train Time |
| ------------------- | ----------------------- | ------------- | ------- | ---------- | -------------- | ------------------------- | ------ | ---------- | -------------- | ------------------------- | ---------- | ---------- | -------------- | ------------------------- | -------- | ---------- | -------------- | ------------------------- | ---------- | ---------- | -------------- | ------------------------- | -------------- | ---------- | -------------- | ------------------------- | ----------- | ---------- | -------------- | ------------------------- | ----------- | ---------- | -------------- | ------------------------- |
| SLM                 | DeBERTa                 | No Aug        | 0.9333  | 116.41     | 2.48           | 113.93                    | 0.8509 | 552.04     | 8.96           | 543.08                    | 0.8216     | 421.58     | 4.9            | 416.68                    | 0.8512   | 828.28     | 19.39          | 808.89                    | 0.8512     | 311.62     | 3.01           | 308.61                    | 0.7873         | 170.6      | 3.73           | 166.87                    | 0.6705      | 136.27     | 2.62           | 133.65                    | 0.5767      | 155.52     | 1.19           | 154.33                    |
| SLM                 | DeBERTa                 | SSMBA         | 0.9217  | 329.39     | 2.62           | 326.77                    | 0.8749 | 3904.47    | 9.52           | 3894.95                   | 0.7966     | 3375.24    | 5.28           | 3369.96                   | 0.8514   | 5007.18    | 21.11          | 4986.07                   | 0.8819     | 1454.25    | 3.21           | 1451.04                   | 0.7963         | 944.46     | 4              | 940.46                    | 0.6335      | 422.52     | 2.78           | 419.74                    | 0.6394      | 586.83     | 1.29           | 585.54                    |
| SLM                 | DeBERTa                 | AEDA          | 0.9362  | 359.68     | 2.6            | 357.08                    | 0.8844 | 3865.92    | 9.35           | 3856.57                   | 0.8182     | 2987.24    | 5.14           | 2982.1                    | 0.8568   | 5134.96    | 20.17          | 5114.79                   | 0.9019     | 1611.72    | 3.06           | 1608.66                   | 0.8001         | 830.23     | 3.95           | 826.28                    | 0.5629      | 385.19     | 2.74           | 382.45                    | 0.7178      | 517.34     | 1.26           | 516.08                    |
| SLM                 | DeBERTa                 | C2L           | 0.942   | 352.34     | 2.7            | 349.64                    | 0.8782 | 5939.3     | 9.79           | 5929.51                   | 0.8479     | 7558.29    | 5.29           | 7553                      | 0.8693   | 8163.71    | 21.46          | 8142.25                   | 0.89       | 1774.71    | 3.29           | 1771.42                   | 0.8031         | 1143.9     | 4.2            | 1139.7                    | 0.6803      | 535.45     | 2.92           | 532.53                    | 0.6764      | 739.21     | 1.32           | 737.89                    |
| SLM                 | DeBERTa                 | TextSmoothing | 0.942   | 299.15     | 2.57           | 296.58                    | 0.8856 | 2098.74    | 8.99           | 2089.75                   | 0.8404     | 2535.61    | 4.95           | 2530.66                   | 0.8637   | 5150.22    | 19.64          | 5130.58                   | 0.9031     | 1309.9     | 3.06           | 1306.84                   | 0.8105         | 899.78     | 3.88           | 895.9                     | 0.6627      | 411.25     | 2.73           | 408.52                    | 0.6271      | 560.97     | 1.24           | 559.73                    |
| SLM                 | XLNet                   | No Aug        | 0.8406  | 144.77     | 5.72           | 139.05                    | 0.8057 | 970.48     | 19.09          | 951.39                    | 0.8236     | 568.05     | 10.35          | 557.7                     | 0.8717   | 862.11     | 26.83          | 835.28                    | 0.8344     | 448.99     | 7.93           | 441.06                    | 0.6102         | 221.13     | 7.7            | 213.43                    | 0.5063      | 179.52     | 6.15           | 173.37                    | 0.5772      | 155.16     | 4.28           | 150.88                    |
| SLM                 | XLNet                   | SSMBA         | 0.8841  | 379.74     | 6.07           | 373.67                    | 0.859  | 4534.03    | 20.49          | 4513.54                   | 0.8098     | 3169.27    | 11.15          | 3158.12                   | 0.8039   | 5876.87    | 28.14          | 5848.73                   | 0.7888     | 1695.58    | 8.26           | 1687.32                   | 0.7406         | 972.27     | 8.23           | 964.04                    | 0.5917      | 443.38     | 6.48           | 436.9                     | 0.4724      | 601.24     | 4.65           | 596.59                    |
| SLM                 | XLNet                   | AEDA          | 0.9275  | 439.44     | 5.9            | 433.54                    | 0.8842 | 4116.05    | 19.93          | 4096.12                   | 0.8341     | 3162.24    | 10.92          | 3151.32                   | 0.7882   | 5450.04    | 28.05          | 5421.99                   | 0.8618     | 1709.64    | 8.26           | 1701.38                   | 0.7225         | 866.95     | 7.81           | 859.14                    | 0.4856      | 395.35     | 6.24           | 389.11                    | 0.5872      | 536.08     | 4.44           | 531.64                    |
| SLM                 | XLNet                   | C2L           | 0.9275  | 552.42     | 6.3            | 546.12                    | 0.8729 | 4750.83    | 20.91          | 4729.92                   | 0.828      | 8773.1     | 11.51          | 8761.59                   | 0.8618   | 4832.82    | 28.39          | 4804.43                   | 0.8756     | 2282.28    | 8.34           | 2273.94                   | 0.7575         | 1190.41    | 8.56           | 1181.85                   | 0.5787      | 551.08     | 6.5            | 544.58                    | 0.6215      | 380.23     | 4.72           | 375.51                    |
| SLM                 | XLNet                   | TextSmoothing | 0.9565  | 386.37     | 5.9            | 380.47                    | 0.8719 | 3611.52    | 20.03          | 3591.49                   | 0.801      | 3511.28    | 10.7           | 3500.58                   | 0.8445   | 5535.39    | 27.08          | 5508.31                   | 0.8869     | 1748.41    | 8.17           | 1740.24                   | 0.823          | 937.83     | 7.79           | 930.04                    | 0.5959      | 422.1      | 6.2            | 415.9                     | 0.6231      | 576.44     | 4.28           | 572.16                    |
| SLM                 | T5                      | No Aug        | 0.6087  | 337.33     | 13.82          | 323.51                    | 0.8713 | 691.76     | 27.77          | 663.99                    | 0.8533     | 594.66     | 12.72          | 581.94                    | 0.8706   | 925.18     | 37.99          | 887.19                    | 0.85       | 497.01     | 7.47           | 489.54                    | 0.7747         | 461.9      | 10.04          | 451.86                    | 0.5185      | 314.27     | 7.21           | 307.06                    | 0.531       | 332.07     | 6.46           | 325.61                    |
| SLM                 | T5                      | SSMBA         | 0.9391  | 477.12     | 14.73          | 462.39                    | 0.8936 | 1905.29    | 29             | 1876.29                   | 0.8439     | 1580       | 13.72          | 1566.28                   | 0.8685   | 2708.44    | 40.14          | 2668.3                    | 0.8756     | 1071.58    | 7.85           | 1063.73                   | 0.7959         | 642.73     | 10.76          | 631.97                    | 0.6569      | 353.56     | 7.63           | 345.93                    | 0.566       | 427.45     | 6.98           | 420.47                    |
| SLM                 | T5                      | AEDA          | 0.9565  | 426.41     | 14.49          | 411.92                    | 0.8823 | 1487.15    | 28.12          | 1459.03                   | 0.8381     | 1275.94    | 13.34          | 1262.6                    | 0.8678   | 1968.18    | 39.63          | 1928.55                   | 0.8688     | 853.87     | 7.7            | 846.17                    | 0.7957         | 530.49     | 10.23          | 520.26                    | 0.6374      | 328.56     | 7.61           | 320.95                    | 0.6337      | 385.7      | 6.6            | 379.1                     |
| SLM                 | T5                      | C2L           | 0.9362  | 622.55     | 14.81          | 607.74                    | 0.893  | 2537.7     | 29.71          | 2507.99                   | 0.8412     | 5688.47    | 14.25          | 5674.22                   | 0.8693   | 3948.24    | 40.33          | 3907.91                   | 0.8737     | 1674.92    | 8.06           | 1666.86                   | 0.7324         | 637.99     | 11.29          | 626.7                     | 0.6723      | 368.08     | 7.89           | 360.19                    | 0.5146      | 738.04     | 7.12           | 730.92                    |
| SLM                 | T5                      | TextSmoothing | 0.942   | 353.27     | 14.18          | 339.09                    | 0.8853 | 1204.93    | 28.77          | 1176.16                   | 0.8533     | 1519.33    | 13.28          | 1506.05                   | 0.8712   | 2093.44    | 39.35          | 2054.09                   | 0.8719     | 820.18     | 7.62           | 812.56                    | 0.7901         | 592.92     | 10.12          | 582.8                     | 0.6561      | 321.54     | 7.54           | 314                       | 0.5926      | 370.77     | 6.65           | 364.12                    |
| LLM Fine-tuning     | CodeGen2.5              | No Aug        | 0.6745  | 79.31      | 1.48           | 77.83                     | 0.5991 | 1079.5     | 15.94          | 1063.56                   | 0.3492     | 674.43     | 8.05           | 666.38                    | 0.4492   | 1441.87    | 18.49          | 1423.38                   | 0.2594     | 350.56     | 3.74           | 346.82                    | 0.4533         | 182.16     | 2.05           | 180.11                    | 0.3436      | 86.06      | 1.37           | 84.69                     | 0.3923      | 114.61     | 1.94           | 112.67                    |
| LLM Fine-tuning     | CodeGen2.5              | SSMBA         | 0.5024  | 428.99     | 1.6            | 427.39                    | 0.8477 | 5046.95    | 16.85          | 5030.1                    | 0.6888     | 3712.57    | 8.38           | 3704.19                   | 0.5259   | 6723.11    | 19.89          | 6703.22                   | 0.7844     | 2021.02    | 3.9            | 2017.12                   | 0.4923         | 1016.24    | 2.23           | 1014.01                   | 0.4857      | 452.9      | 1.43           | 451.47                    | 0.5         | 643.94     | 2.1            | 641.84                    |
| LLM Fine-tuning     | CodeGen2.5              | AEDA          | 0.6023  | 429.16     | 1.5            | 427.66                    | 0.8734 | 5043.22    | 16.44          | 5026.78                   | 0.0998     | 3815.96    | 8.3            | 3807.66                   | 0.8397   | 6890.1     | 19.08          | 6871.02                   | 0.7531     | 2006.45    | 3.82           | 2002.63                   | 0.4737         | 1024.48    | 2.16           | 1022.32                   | 0.4786      | 479.79     | 1.43           | 478.36                    | 0.4906      | 643.11     | 2.01           | 641.1                     |
| LLM Fine-tuning     | CodeGen2.5              | C2L           | 0.5378  | 560.88     | 1.61           | 559.27                    | 0.8658 | 6545.29    | 17.48          | 6527.81                   | 0.4097     | 4873.74    | 8.55           | 4865.19                   | 0.8318   | 8973.53    | 20.34          | 8953.19                   | 0.8181     | 2629.92    | 3.91           | 2626.01                   | 0.4934         | 1359.6     | 2.24           | 1357.36                   | 0.4765      | 606.67     | 1.5            | 605.17                    | 0.4532      | 857.62     | 2.17           | 855.45                    |
| LLM Fine-tuning     | CodeGen2.5              | TextSmoothing | 0.4334  | 431.93     | 1.5            | 430.43                    | 0.7641 | 5061.96    | 16.07          | 5045.89                   | 0.574      | 3882.58    | 8.18           | 3874.4                    | 0.5915   | 6921.46    | 18.83          | 6902.63                   | 0.7644     | 2007.37    | 3.83           | 2003.54                   | 0.4684         | 1031.28    | 2.14           | 1029.14                   | 0.5006      | 467.21     | 1.42           | 465.79                    | 0.5006      | 643.36     | 1.97           | 641.39                    |
| LLM Fine-tuning     | Phi-2                   | No Aug        | 0.8899  | 37.7       | 0.87           | 36.83                     | 0.7357 | 379.09     | 5.38           | 373.71                    | 0.1092     | 336.97     | 5.29           | 331.68                    | 0.7784   | 562.9      | 6.58           | 556.32                    | 0.5231     | 165.86     | 2.21           | 163.65                    | 0.4765         | 96.88      | 0.88           | 96                        | 0.453       | 43.95      | 0.88           | 43.07                     | 0.4815      | 59.67      | 0.79           | 58.88                     |
| LLM Fine-tuning     | Phi-2                   | SSMBA         | 0.8203  | 247.58     | 0.94           | 246.64                    | 0.8525 | 2898.23    | 5.84           | 2892.39                   | 0.5475     | 2108.43    | 5.72           | 2102.71                   | 0.8387   | 4153.06    | 6.96           | 4146.1                    | 0.765      | 1170.55    | 2.33           | 1168.22                   | 0.5538         | 712.91     | 0.95           | 711.96                    | 0.4565      | 277.79     | 0.95           | 276.84                    | 0.4382      | 422.97     | 0.85           | 422.12                    |
| LLM Fine-tuning     | Phi-2                   | AEDA          | 0.9217  | 248.48     | 0.92           | 247.56                    | 0.8481 | 2874.56    | 5.69           | 2868.87                   | 0.4981     | 2162.31    | 5.5            | 2156.81                   | 0.8488   | 4174.58    | 6.77           | 4167.81                   | 0.8106     | 1153.38    | 2.3            | 1151.08                   | 0.4045         | 649.92     | 0.92           | 649                       | 0.4423      | 320.34     | 0.89           | 319.45                    | 0.5633      | 407.99     | 0.83           | 407.16                    |
| LLM Fine-tuning     | Phi-2                   | C2L           | 0.931   | 324.8      | 0.98           | 323.82                    | 0.8403 | 3838.23    | 6.04           | 3832.19                   | 0.6        | 2978.16    | 5.95           | 2972.21                   | 0.8532   | 5703       | 7.16           | 5695.84                   | 0.6925     | 1539.82    | 2.37           | 1537.45                   | 0.5248         | 829.41     | 0.98           | 828.43                    | 0.4905      | 392.05     | 0.99           | 391.06                    | 0.58        | 538.34     | 0.88           | 537.46                    |
| LLM Fine-tuning     | Phi-2                   | TextSmoothing | 0.9188  | 249.64     | 0.9            | 248.74                    | 0.8362 | 2906.1     | 5.53           | 2900.57                   | 0.595      | 2155.4     | 5.34           | 2150.06                   | 0.8401   | 4229.53    | 6.86           | 4222.67                   | 0.8256     | 1211.56    | 2.31           | 1209.25                   | 0.5701         | 644.73     | 0.9            | 643.83                    | 0.4803      | 299.58     | 0.9            | 298.68                    | 0.4725      | 392.86     | 0.81           | 392.05                    |
| LLM Fine-tuning     | DeepSeek                | No Aug        | 0.8899  | 53.42      | 1.29           | 52.13                     | 0.6991 | 657.44     | 8.61           | 648.83                    | 0.0124     | 447.69     | 3.83           | 443.86                    | 0.603    | 855.91     | 9.37           | 846.54                    | 0.7287     | 237.03     | 3.15           | 233.88                    | 0.5159         | 123.59     | 1.47           | 122.12                    | 0.3171      | 58.36      | 1.63           | 56.73                     | 0.4416      | 78.61      | 1.86           | 76.75                     |
| LLM Fine-tuning     | DeepSeek                | SSMBA         | 0.8986  | 375.17     | 1.37           | 373.8                     | 0.7865 | 4278.92    | 8.98           | 4269.94                   | 0.3223     | 3138.11    | 4.1            | 3134.01                   | 0.5788   | 5741.04    | 10.12          | 5730.92                   | 0.7644     | 1766.2     | 3.31           | 1762.89                   | 0.4808         | 889.39     | 1.54           | 887.85                    | 0.3347      | 395.25     | 1.72           | 393.53                    | 0.4931      | 558.07     | 2.03           | 556.04                    |
| LLM Fine-tuning     | DeepSeek                | AEDA          | 0.8435  | 374.65     | 1.32           | 373.33                    | 0.7494 | 4277.57    | 9.05           | 4268.52                   | 0.0165     | 3362.25    | 3.95           | 3358.3                    | 0.5514   | 5864.69    | 9.91           | 5854.78                   | 0.76       | 1767.99    | 3.21           | 1764.78                   | 0.32           | 901.12     | 1.54           | 899.58                    | 0.3133      | 407.42     | 1.7            | 405.72                    | 0.4331      | 558.08     | 1.89           | 556.19                    |
| LLM Fine-tuning     | DeepSeek                | C2L           | 0.858   | 495.94     | 1.37           | 494.57                    | 0.765  | 5646.48    | 9.14           | 5637.34                   | 0.3233     | 4307.61    | 4.11           | 4303.5                    | 0.3445   | 7597.7     | 10.22          | 7587.48                   | 0.7619     | 2337.33    | 3.34           | 2333.99                   | 0.4277         | 1191.9     | 1.6            | 1190.3                    | 0.417       | 534.89     | 1.73           | 533.16                    | 0.451       | 737.87     | 2.12           | 735.75                    |
| LLM Fine-tuning     | DeepSeek                | TextSmoothing | 0.9101  | 375.55     | 1.31           | 374.24                    | 0.7238 | 4323.1     | 8.74           | 4314.36                   | 0.5812     | 3299.06    | 3.99           | 3295.07                   | 0.6388   | 5890.86    | 9.79           | 5881.07                   | 0.76       | 1762.99    | 3.3            | 1759.69                   | 0.4419         | 900.8      | 1.48           | 899.32                    | 0.3904      | 407.81     | 1.64           | 406.17                    | 0.4976      | 558.92     | 1.89           | 557.03                    |
| LLM Prompting       | GPT-4.1 nano (baseline) | baseline      | 0.8841  | 29.97      | 29.97          | 0                         | 0.5117 | 585.85     | 585.85         | 0                         | 0.3283     | 256.25     | 256.25         | 0                         | 0.3783   | 752.44     | 752.44         | 0                         | 0.5156     | 140.79     | 140.79         | 0                         | 0.5708         | 73.59      | 73.59          | 0                         | 0.4878      | 32.22      | 32.22          | 0                         | 0.5714      | 41.71      | 41.71          | 0                         |
| LLM Prompting (k=1) | GPT-4.1 nano            | No Aug        | 0.9275  | 29.97      | 29.97          | 0                         | 0.5072 | 585.91     | 585.91         | 0                         | 0.2626     | 256.25     | 256.25         | 0                         | 0.4255   | 752.44     | 752.44         | 0                         | 0.6188     | 140.79     | 140.79         | 0                         | 0.5477         | 73.59      | 73.59          | 0                         | 0.4785      | 32.22      | 32.22          | 0                         | 0.5541      | 41.71      | 41.71          | 0                         |
| LLM Prompting (k=1) | GPT-4.1 nano            | SSMBA         | 0.8261  | 33.99      | 30             | 3.99                      | 0.437  | 589.93     | 585.94         | 3.99                      | 0.1178     | 260.27     | 256.26         | 4.01                      | 0.4118   | 756.46     | 752.39         | 4.07                      | 0.3375     | 144.81     | 140.72         | 4.09                      | 0.4734         | 77.61      | 73.44          | 4.17                      | 0.3436      | 36.24      | 32.05          | 4.19                      | 0.5         | 45.73      | 41.44          | 4.29                      |
| LLM Prompting (k=1) | GPT-4.1 nano            | AEDA          | 0.8696  | 29.99      | 29.98          | 0.01                      | 0.5509 | 585.93     | 585.82         | 0.11                      | 0.2054     | 256.27     | 256.09         | 0.18                      | 0.471    | 752.46     | 752.27         | 0.19                      | 0.5375     | 140.81     | 140.61         | 0.2                       | 0.5721         | 73.61      | 73.34          | 0.27                      | 0.454       | 32.24      | 31.95          | 0.29                      | 0.417       | 41.73      | 41.37          | 0.36                      |
| LLM Prompting (k=1) | GPT-4.1 nano            | C2L           | 0.8654  | 36.66      | 30             | 6.66                      | 0.5399 | 592.6      | 585.86         | 6.74                      | 0.3142     | 262.94     | 256.18         | 6.76                      | 0.4194   | 759.13     | 752.28         | 6.85                      | 0.5782     | 147.48     | 140.62         | 6.86                      | 0.5203         | 80.28      | 73.4           | 6.88                      | 0.3984      | 38.91      | 32.02          | 6.89                      | 0.5073      | 48.4       | 41.49          | 6.91                      |
| LLM Prompting (k=1) | GPT-4.1 nano            | TextSmoothing | 0.8696  | 33.19      | 30.01          | 3.18                      | 0.4717 | 589.13     | 585.94         | 3.19                      | 0.2037     | 259.47     | 256.21         | 3.26                      | 0.438    | 755.66     | 752.39         | 3.27                      | 0.5687     | 144.01     | 140.67         | 3.34                      | 0.5337         | 76.81      | 73.4           | 3.41                      | 0.4417      | 35.44      | 32             | 3.44                      | 0.5873      | 44.93      | 41.45          | 3.48                      |
| LLM Prompting (k=2) | GPT-4.1 nano            | No Aug        | 0.913   | 29.97      | 29.97          | 0                         | 0.5155 | 585.97     | 585.97         | 0                         | 0.3704     | 256.25     | 256.25         | 0                         | 0.4471   | 752.44     | 752.44         | 0                         | 0.5844     | 140.79     | 140.79         | 0                         | 0.5631         | 73.59      | 73.59          | 0                         | 0.5333      | 32.22      | 32.22          | 0                         | 0.453       | 41.71      | 41.71          | 0                         |
| LLM Prompting (k=2) | GPT-4.1 nano            | SSMBA         | 0.8116  | 33.99      | 29.98          | 4.01                      | 0.3291 | 589.99     | 585.89         | 4.1                       | 0.2273     | 260.27     | 256.11         | 4.16                      | 0.4534   | 756.46     | 752.29         | 4.17                      | 0.3344     | 144.81     | 140.61         | 4.2                       | 0.4963         | 77.61      | 73.38          | 4.23                      | 0.3804      | 36.24      | 31.99          | 4.25                      | 0.2982      | 45.73      | 41.47          | 4.26                      |
| LLM Prompting (k=2) | GPT-4.1 nano            | AEDA          | 0.8696  | 29.99      | 29.97          | 0.02                      | 0.4015 | 585.99     | 585.95         | 0.04                      | 0.2374     | 256.27     | 256.22         | 0.05                      | 0.4022   | 752.46     | 752.38         | 0.08                      | 0.4781     | 140.81     | 140.72         | 0.09                      | 0.4521         | 73.61      | 73.52          | 0.09                      | 0.4146      | 32.24      | 32.13          | 0.11                      | 0.3509      | 41.73      | 41.61          | 0.12                      |
| LLM Prompting (k=2) | GPT-4.1 nano            | C2L           | 0.8551  | 36.66      | 30.01          | 6.65                      | 0.403  | 592.66     | 586.01         | 6.65                      | 0.2323     | 262.94     | 256.2          | 6.74                      | 0.4082   | 759.13     | 752.3          | 6.83                      | 0.55       | 147.48     | 140.64         | 6.84                      | 0.3391         | 80.28      | 73.35          | 6.93                      | 0.3537      | 38.91      | 31.9           | 7.01                      | 0.3777      | 48.4       | 41.37          | 7.03                      |
| LLM Prompting (k=2) | GPT-4.1 nano            | TextSmoothing | 0.8841  | 33.19      | 30.01          | 3.18                      | 0.3819 | 589.19     | 586.01         | 3.18                      | 0.2205     | 259.47     | 256.24         | 3.23                      | 0.413    | 755.66     | 752.41         | 3.25                      | 0.5281     | 144.01     | 140.76         | 3.25                      | 0.5294         | 76.81      | 73.55          | 3.26                      | 0.4756      | 35.44      | 32.11          | 3.33                      | 0.5267      | 44.93      | 41.52          | 3.41                      |
| LLM Prompting (k=4) | GPT-4.1 nano            | No Aug        | 0.8551  | 30         | 30             | 0                         | 0.5223 | 585.85     | 585.81         | 0.04                      | 0.3333     | 256.25     | 256.15         | 0.1                       | 0.4664   | 752.44     | 752.24         | 0.2                       | 0.3625     | 140.79     | 140.59         | 0.2                       | 0.6499         | 73.59      | 73.39          | 0.2                       | 0.5854      | 32.22      | 31.99          | 0.23                      | 0.3406      | 41.71      | 41.42          | 0.29                      |
| LLM Prompting (k=4) | GPT-4.1 nano            | SSMBA         | 0.8841  | 34.02      | 30.03          | 3.99                      | 0.5019 | 589.87     | 585.81         | 4.06                      | 0.3838     | 260.27     | 256.17         | 4.1                       | 0.5051   | 756.46     | 752.32         | 4.14                      | 0.6188     | 144.81     | 140.66         | 4.15                      | 0.4545         | 77.61      | 73.38          | 4.23                      | 0.4294      | 36.24      | 31.97          | 4.27                      | 0.3144      | 45.73      | 41.36          | 4.37                      |
| LLM Prompting (k=4) | GPT-4.1 nano            | AEDA          | 0.8406  | 30.02      | 30.01          | 0.01                      | 0.5049 | 585.87     | 585.8          | 0.07                      | 0.4562     | 256.27     | 256.14         | 0.13                      | 0.475    | 752.46     | 752.29         | 0.17                      | 0.6312     | 140.81     | 140.55         | 0.26                      | 0.5085         | 73.61      | 73.29          | 0.32                      | 0.4727      | 32.24      | 31.91          | 0.33                      | 0.3739      | 41.73      | 41.37          | 0.36                      |
| LLM Prompting (k=4) | GPT-4.1 nano            | C2L           | 0.8639  | 36.69      | 30.04          | 6.65                      | 0.5381 | 592.54     | 585.8          | 6.74                      | 0.3754     | 262.94     | 256.12         | 6.82                      | 0.446    | 759.13     | 752.26         | 6.87                      | 0.625      | 147.48     | 140.61         | 6.87                      | 0.5217         | 80.28      | 73.38          | 6.9                       | 0.4785      | 38.91      | 31.91          | 7                         | 0.3319      | 48.4       | 41.34          | 7.06                      |
| LLM Prompting (k=4) | GPT-4.1 nano            | TextSmoothing | 0.8551  | 33.22      | 30.01          | 3.21                      | 0.4891 | 589.07     | 585.86         | 3.21                      | 0.3519     | 259.47     | 256.16         | 3.31                      | 0.4551   | 755.66     | 752.25         | 3.41                      | 0.5906     | 144.01     | 140.56         | 3.45                      | 0.5585         | 76.81      | 73.35          | 3.46                      | 0.5399      | 35.44      | 31.9           | 3.54                      | 0.381       | 44.93      | 41.33          | 3.6                       |
| LLM Prompting       | GPT-5 nano (baseline)   | baseline      | 0.9062  | 142.7      | 142.7          | 0                         | 0.7789 | 2705.58    | 2705.58        | 0                         | 0.7139     | 1264.73    | 1264.73        | 0                         | 0.7258   | 3689.5     | 3689.5         | 0                         | 0.8375     | 756.98     | 756.98         | 0                         | 0.7837         | 569.66     | 569.66         | 0                         | 0.5791      | 268.12     | 268.12         | 0                         | 0.6366      | 380.52     | 380.52         | 0                         |
| LLM Prompting (k=1) | GPT-5 nano              | No Aug        | 0.913   | 142.81     | 142.81         | 0                         | 0.7932 | 2705.58    | 2705.58        | 0                         | 0.7357     | 1264.73    | 1264.73        | 0                         | 0.7486   | 3689.5     | 3689.5         | 0                         | 0.85       | 756.98     | 756.98         | 0                         | 0.781          | 569.66     | 569.66         | 0                         | 0.581       | 268.12     | 268.12         | 0                         | 0.6039      | 380.52     | 380.52         | 0                         |
| LLM Prompting (k=1) | GPT-5 nano              | SSMBA         | 0.913   | 146.83     | 142.74         | 4.09                      | 0.7758 | 2709.6     | 2705.45        | 4.15                      | 0.734      | 1268.75    | 1264.51        | 4.24                      | 0.7691   | 3693.52    | 3689.19        | 4.33                      | 0.8156     | 761        | 756.65         | 4.35                      | 0.7466         | 573.68     | 569.27         | 4.41                      | 0.6034      | 272.14     | 267.71         | 4.43                      | 0.5918      | 384.54     | 380.1          | 4.44                      |
| LLM Prompting (k=1) | GPT-5 nano              | AEDA          | 0.9275  | 142.83     | 142.68         | 0.15                      | 0.7917 | 2705.6     | 2705.42        | 0.18                      | 0.7424     | 1264.75    | 1264.52        | 0.23                      | 0.7692   | 3689.52    | 3689.21        | 0.31                      | 0.8219     | 757        | 756.6          | 0.4                       | 0.7822         | 569.68     | 569.26         | 0.42                      | 0.6298      | 268.14     | 267.7          | 0.44                      | 0.5589      | 380.54     | 380            | 0.54                      |
| LLM Prompting (k=1) | GPT-5 nano              | C2L           | 0.9246  | 149.5      | 142.72         | 6.78                      | 0.7911 | 2712.27    | 2705.43        | 6.84                      | 0.7348     | 1271.42    | 1264.55        | 6.87                      | 0.7684   | 3696.19    | 3689.24        | 6.95                      | 0.85       | 763.67     | 756.63         | 7.04                      | 0.7151         | 576.35     | 569.28         | 7.07                      | 0.6186      | 274.81     | 267.67         | 7.14                      | 0.5871      | 387.21     | 380.04         | 7.17                      |
| LLM Prompting (k=1) | GPT-5 nano              | TextSmoothing | 0.942   | 146.03     | 142.74         | 3.29                      | 0.7826 | 2708.8     | 2705.48        | 3.32                      | 0.7239     | 1267.95    | 1264.58        | 3.37                      | 0.7622   | 3692.72    | 3689.35        | 3.37                      | 0.8406     | 760.2      | 756.79         | 3.41                      | 0.7758         | 572.88     | 569.42         | 3.46                      | 0.6138      | 271.34     | 267.85         | 3.49                      | 0.5863      | 383.74     | 380.21         | 3.53                      |
| LLM Prompting (k=2) | GPT-5 nano              | No Aug        | 9275    | 142.72     | 142.72         | 0                         | 0.797  | 2705.58    | 2705.58        | 0                         | 0.7273     | 1264.73    | 1264.73        | 0                         | 0.7355   | 3689.5     | 3689.5         | 0                         | 0.8719     | 756.98     | 756.98         | 0                         | 0.7805         | 569.66     | 569.66         | 0                         | 0.6369      | 268.12     | 268.12         | 0                         | 0.6496      | 380.52     | 380.52         | 0                         |
| LLM Prompting (k=2) | GPT-5 nano              | SSMBA         | 0.8986  | 146.74     | 142.83         | 3.91                      | 0.7819 | 2709.6     | 2705.59        | 4.01                      | 0.7323     | 1268.75    | 1264.73        | 4.02                      | 0.7645   | 3693.52    | 3689.5         | 4.02                      | 0.825      | 761        | 756.9          | 4.1                       | 0.75           | 573.68     | 569.49         | 4.19                      | 0.6333      | 272.14     | 267.95         | 4.19                      | 0.6111      | 384.54     | 380.26         | 4.28                      |
| LLM Prompting (k=2) | GPT-5 nano              | AEDA          | 0.9431  | 142.74     | 142.73         | 0.01                      | 0.7947 | 2705.6     | 2705.59        | 0.01                      | 0.7542     | 1264.75    | 1264.72        | 0.03                      | 0.76     | 3689.52    | 3689.47        | 0.05                      | 0.85       | 757        | 756.91         | 0.09                      | 0.7692         | 569.68     | 569.49         | 0.19                      | 0.625       | 268.14     | 267.93         | 0.21                      | 0.5992      | 380.54     | 380.24         | 0.3                       |
| LLM Prompting (k=2) | GPT-5 nano              | C2L           | 0.913   | 149.41     | 142.91         | 6.5                       | 0.7781 | 2712.27    | 2705.7         | 6.57                      | 0.7527     | 1271.42    | 1264.8         | 6.62                      | 0.7571   | 3696.19    | 3689.51        | 6.68                      | 0.8469     | 763.67     | 756.92         | 6.75                      | 0.7838         | 576.35     | 569.57         | 6.78                      | 0.5635      | 274.81     | 267.99         | 6.82                      | 0.5932      | 387.21     | 380.32         | 6.89                      |
| LLM Prompting (k=2) | GPT-5 nano              | TextSmoothing | 0.942   | 145.94     | 143.32         | 2.62                      | 0.7849 | 2708.8     | 2706.11        | 2.69                      | 0.7542     | 1267.95    | 1265.23        | 2.72                      | 0.756    | 3692.72    | 3689.92        | 2.8                       | 0.8562     | 760.2      | 757.4          | 2.8                       | 0.7911         | 572.88     | 570.07         | 2.81                      | 0.6444      | 271.34     | 268.49         | 2.85                      | 0.5882      | 383.74     | 380.86         | 2.88                      |
| LLM Prompting (k=4) | GPT-5 nano              | No Aug        | 0.9412  | 142.77     | 142.77         | 0                         | 0.7849 | 2705.58    | 2705.58        | 0                         | 0.7475     | 1264.73    | 1264.73        | 0                         | 0.7457   | 3689.5     | 3689.5         | 0                         | 0.8481     | 756.98     | 756.98         | 0                         | 0.8017         | 569.66     | 569.66         | 0                         | 0.6374      | 268.12     | 268.12         | 0                         | 0.6178      | 380.52     | 380.52         | 0                         |
| LLM Prompting (k=4) | GPT-5 nano              | SSMBA         | 0.9259  | 146.79     | 143.27         | 3.52                      | 0.7623 | 2709.6     | 2706.04        | 3.56                      | 0.7559     | 1268.75    | 1265.16        | 3.59                      | 0.7833   | 3693.52    | 3689.91        | 3.61                      | 0.8469     | 761        | 757.34         | 3.66                      | 0.7838         | 573.68     | 570            | 3.68                      | 0.5909      | 272.14     | 268.41         | 3.73                      | 0.5932      | 384.54     | 380.79         | 3.75                      |
| LLM Prompting (k=4) | GPT-5 nano              | AEDA          | 0.9275  | 142.79     | 142.78         | 0.01                      | 0.7743 | 2705.6     | 2705.52        | 0.08                      | 0.771      | 1264.75    | 1264.66        | 0.09                      | 0.7685   | 3689.52    | 3689.37        | 0.15                      | 0.8219     | 757        | 756.77         | 0.23                      | 0.782          | 569.68     | 569.36         | 0.32                      | 0.5682      | 268.14     | 267.79         | 0.35                      | 0.575       | 380.54     | 380.13         | 0.41                      |
| LLM Prompting (k=4) | GPT-5 nano              | C2L           | 0.942   | 149.46     | 144.24         | 5.22                      | 0.7592 | 2712.27    | 2706.96        | 5.31                      | 0.7508     | 1271.42    | 1266.08        | 5.34                      | 0.7679   | 3696.19    | 3690.78        | 5.41                      | 0.8375     | 763.67     | 758.23         | 5.44                      | 0.7875         | 576.35     | 570.86         | 5.49                      | 0.6448      | 274.81     | 269.29         | 5.52                      | 0.5633      | 387.21     | 381.67         | 5.54                      |
| LLM Prompting (k=4) | GPT-5 nano              | TextSmoothing | 0.9363  | 145.99     | 142.89         | 3.1                       | 0.7615 | 2708.8     | 2705.66        | 3.14                      | 0.7407     | 1267.95    | 1264.79        | 3.16                      | 0.7656   | 3692.72    | 3689.54        | 3.18                      | 0.8187     | 760.2      | 757            | 3.2                       | 0.7871         | 572.88     | 569.61         | 3.27                      | 0.5989      | 271.34     | 268.04         | 3.3                       | 0.582       | 383.74     | 380.37         | 3.37                      |